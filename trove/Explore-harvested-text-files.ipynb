{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore harvested text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fileinput\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_harvest():\n",
    "    '''\n",
    "    Get the timestamp of the most recent harvest.\n",
    "    '''\n",
    "    harvests = sorted([d for d in os.listdir('data') if os.path.isdir(os.path.join('data', d))])\n",
    "    return harvests[-1]\n",
    "\n",
    "def open_harvest_data(timestamp=None):\n",
    "    '''\n",
    "    Open the results of the specified harvest (most recent by default).\n",
    "    \n",
    "    Returns a DataFrame.\n",
    "    '''\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    print(timestamp)\n",
    "    df = pd.read_csv(os.path.join('data', timestamp, 'results.csv'), parse_dates=['date'])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_texts(timestamp=None):\n",
    "    '''\n",
    "    Aggregate all individual article texts creating one big file.\n",
    "    '''\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    output_file = os.path.join('data', timestamp, 'all-texts.txt')\n",
    "    data_dir = os.path.join('data', timestamp, 'text')\n",
    "    files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file[-4:] == '.txt']                                                                                   \n",
    "    with open(output_file, 'w') as fout, fileinput.input(files) as fin:\n",
    "        for line in fin:\n",
    "            fout.write(line)\n",
    "    \n",
    "def aggregate_years(timestamp=None):\n",
    "    '''\n",
    "    Aggregate individual article text by year, creating one file per year.\n",
    "    '''\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    output_dir = os.path.join('data', timestamp, 'years')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data_dir = os.path.join('data', timestamp, 'text')\n",
    "    df = open_harvest_data(timestamp=timestamp)\n",
    "    df['year'] = df['date'].dt.year\n",
    "    years = list(df['year'].unique())\n",
    "    for year in years:\n",
    "        output_file = os.path.join(output_dir, '{}.txt'.format(year))\n",
    "        files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file[-4:] == '.txt' and file[:4] == str(year)]                                                                                   \n",
    "        with open(output_file, 'w') as fout, fileinput.input(files) as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "                \n",
    "                \n",
    "def aggregate_newspapers(timestamp=None):\n",
    "    '''\n",
    "    Aggregate individual article text by newspaper, creating one file per newspaper.\n",
    "    '''\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    output_dir = os.path.join('data', timestamp, 'newspapers')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data_dir = os.path.join('data', timestamp, 'text')\n",
    "    df = open_harvest_data(timestamp=timestamp)\n",
    "    newspapers = list(df['newspaper_id'].unique())\n",
    "    for newspaper in newspapers:\n",
    "        output_file = os.path.join(output_dir, '{}.txt'.format(newspaper))\n",
    "        files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file[-4:] == '.txt' and '-{}-'.format(newspaper) in file]                                                                                   \n",
    "        with open(output_file, 'w') as fout, fileinput.input(files) as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_frequencies(text_file):\n",
    "    with open(text_file, 'r') as text:\n",
    "        blob = TextBlob(text.read())\n",
    "    word_counts = [[word, count] for word, count in blob.lower().word_counts.items() if word not in stopwords]\n",
    "    word_counts = sorted(word_counts, key=itemgetter(1), reverse=True)[:25]\n",
    "    return pd.DataFrame(word_counts).style.format({1: '{:,}'}).bar(subset=[1], color='#d65f5f').set_properties(subset=[1], **{'width': '300px'})\n",
    "\n",
    "def word_frequency_all(timestamp=None):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    data_dir = os.path.join('data', timestamp)\n",
    "    return show_word_frequencies(os.path.join(data_dir, 'all-texts.txt'))\n",
    "\n",
    "def word_frequency_by_year(year, timestamp=None):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    data_dir = os.path.join('data', timestamp, 'years')\n",
    "    return show_word_frequencies(os.path.join(data_dir, '{}.txt'.format(year)))\n",
    "\n",
    "def word_frequency_by_newspaper(newspaper_id, timestamp=None):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    data_dir = os.path.join('data', timestamp, 'newspapers')\n",
    "    return show_word_frequencies(os.path.join(data_dir, '{}.txt'.format(year)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(names, files, ngram_size=1):\n",
    "    # Chomp chomp -- getting trigrams\n",
    "    tf = TfidfVectorizer(input='filename', analyzer='word', ngram_range=(ngram_size, ngram_size), min_df=0, smooth_idf=False, sublinear_tf=True)\n",
    "    tfidf_matrix = tf.fit_transform(files)\n",
    "    # These are the actual phrases\n",
    "    feature_names = tf.get_feature_names()\n",
    "    # These are the scores\n",
    "    texts = tfidf_matrix.todense()\n",
    "    for index, row in enumerate(texts):\n",
    "        name = names[index]\n",
    "        print('\\n\\n{}\\n'.format(name.upper()))\n",
    "        text = row.tolist()[0]\n",
    "        # If the score is not 0 save it with an index (which will let us get the feature_name)\n",
    "        scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]\n",
    "        sorted_scores = sorted(scores, key=lambda t: t[1] * -1)\n",
    "        # Print the top 20 results for each file\n",
    "        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_scores][:20]:\n",
    "            print('{0: <40} {1}'.format(phrase, score))\n",
    "\n",
    "def calculate_tfidf_by_year(timestamp=None, ngram_size=1):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    data_dir = os.path.join('data', timestamp, 'years')\n",
    "    if not os.path.exists(data_dir):\n",
    "        aggregate_years(timestamp)\n",
    "    # Get a list of the file names in the directory.\n",
    "    names = [file[:-4] for file in os.listdir(data_dir) if file[-4:] == '.txt']\n",
    "    # Get a list of files to feed to scikit-learn.\n",
    "    files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file[-4:] == '.txt']\n",
    "    calculate_tfidf(names, files, ngram_size)\n",
    "    \n",
    "    \n",
    "def calculate_tfidf_by_newspaper(timestamp=None, ngram_size=1):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_harvest()\n",
    "    data_dir = os.path.join('data', timestamp, 'newspapers')\n",
    "    if not os.path.exists(data_dir):\n",
    "        aggregate_newspapers(timestamp)\n",
    "    df = open_harvest_data(timestamp=timestamp)\n",
    "    newspapers = df[['newspaper_id', 'newspaper_title']].drop_duplicates().set_index('newspaper_id')\n",
    "    # Get a list of the file names in the directory.\n",
    "    names = [newspapers.loc[int(file[:-4])]['newspaper_title'] for file in os.listdir(data_dir) if file[-4:] == '.txt']\n",
    "    # Get a list of files to feed to scikit-learn.\n",
    "    files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file[-4:] == '.txt']\n",
    "    calculate_tfidf(names, files, ngram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tfidf_by_newspaper(ngram_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
